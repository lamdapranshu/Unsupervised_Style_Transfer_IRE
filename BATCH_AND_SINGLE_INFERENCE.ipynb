{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "W76J6MPGtre8"
      },
      "outputs": [],
      "source": [
        "# Inference; Given a sentence and target style, transfer the style"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------- IMP -------------\n",
        "# Change value of DEVICE_1 to 1 when running in kaggle"
      ],
      "metadata": {
        "id": "l-h9YgpD9nTo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jm-XwPjr3ekR",
        "outputId": "5c8ea09d-50ac-4961-974d-cbb594cec7fb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install profanity_filter # ----- CHECK THIS -------"
      ],
      "metadata": {
        "id": "6vnLTorFBaoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cmake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bgrpz4AhB_De",
        "outputId": "365e2cd8-4381-4fbd-8f1a-433421d2ddbf"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.27.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desired imports\n",
        "import torch\n",
        "from transformers import (WEIGHTS_NAME, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer)\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "1u7a6ODr3BGf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Directories\n",
        "final_invp_paraphrase_root_dir = \"/content/drive/MyDrive/IRE/INVP/final_INVP\"\n",
        "final_dvp_paraphrase_root_dir = \"/content/drive/MyDrive/IRE/DVP/final_DVP\"\n",
        "src_data_samples_dir = \"/content/drive/MyDrive/IRE/SOURCE_SAMPLES\"\n",
        "final_style_transferred_dir = \"/content/drive/MyDrive/IRE/STYLE_TRANSFERRED\"\n",
        "\n",
        "# Target style that are only possible; since inverse paraphraser models are trained only\n",
        "possible_target_style = {\n",
        "    \"shakespeare\" : \"shakespeare\", # style : invp_model_dir\n",
        "    \"poetry\" : \"poetry\",\n",
        "    \"bible\" : \"bible\"\n",
        "}\n",
        "\n",
        "# Root dir for source style data_samples\n",
        "src_data_samples_fname = { # style:fname_to_read\n",
        "    \"shakespeare\" : \"shakespeare.txt\",\n",
        "    \"poetry\" : \"romantic_poetry.txt\",\n",
        "    \"bible\" : \"bible.txt\",\n",
        "    \"aae\" : \"aae.txt\",\n",
        "    \"joyce\" : \"joyce.txt\",\n",
        "    \"switchboard\" : \"switchboard.txt\",\n",
        "    \"english_tweets\" : \"english_tweets.txt\"\n",
        "}"
      ],
      "metadata": {
        "id": "U6Hk_1Naz_GP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpus = torch.cuda.device_count()\n",
        "\n",
        "print(\"Device- \", device)\n",
        "print(\"No. of GPUs- \", n_gpus)\n",
        "\n",
        "DEVICE_0 = 0\n",
        "DEVICE_1 = 0 # change here to 1 when run in kaggle; enables inverse paraphraser to load in other GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KWUdDEp00rd",
        "outputId": "92a0cc26-6afd-4d75-ee8b-1606fabb2261"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device-  cuda\n",
            "No. of GPUs-  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model & tokenizer type that will be used to load models\n",
        "MODEL_CLASSES = {\n",
        "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
        "}\n",
        "_, model_class, tokenizer_class = MODEL_CLASSES[\"gpt2\"]\n",
        "\n",
        "print(\"GPT2 Model class- \", model_class)\n",
        "print(\"GPT2 Tokenizer class- \", tokenizer_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FMMemi03WA7",
        "outputId": "c387d609-b100-49ce-8cb9-39507a926be1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2 Model class-  <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
            "GPT2 Tokenizer class-  <class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dvp_on_device = DEVICE_0\n",
        "invp_on_device = DEVICE_1"
      ],
      "metadata": {
        "id": "uo1LW-gIUXFc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load DVP\n",
        "with torch.cuda.device(dvp_on_device):\n",
        "  print(\"Loading Diverse Paraphraser...\")\n",
        "  dvp_chkpoint = final_dvp_paraphrase_root_dir\n",
        "  dvp_paraphraser = model_class.from_pretrained(dvp_chkpoint)\n",
        "  dvp_tokenizer = tokenizer_class.from_pretrained(dvp_chkpoint, do_lower_case = True)\n",
        "\n",
        "  dvp_paraphraser.to(torch.cuda.current_device())\n",
        "\n",
        "  print(\"Done loading Diverse Paraphraser..!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HI7qehOV3F2h",
        "outputId": "ac094cdd-cfc4-432c-b009-b8ac8140b1b5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Diverse Paraphraser...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done loading Diverse Paraphraser..!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Init configs\n",
        "\n",
        "MAX_PARAPHRASE_LEN = 100\n",
        "\n",
        "# mainly to handle input\n",
        "CONFIG = {\n",
        "    \"max_prefix_length\": int(MAX_PARAPHRASE_LEN / 2),\n",
        "    \"max_suffix_length\": int(MAX_PARAPHRASE_LEN / 2)\n",
        "}"
      ],
      "metadata": {
        "id": "mway_mOqLBZ6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do Nucleas sampling and greedy decoding; vary top_p to consider more diversity\n",
        "def top_k_top_p_filtering(logits, top_k = 0, top_p = 0.0, filter_amt = -float('Inf')):\n",
        "  top_k = min(top_k, logits.size(-1))  # tok_k should be less than total vocab size\n",
        "\n",
        "  if top_p > 0.0:\n",
        "      sorted_logits, sorted_idxs = torch.sort(logits, descending = True)\n",
        "      cumul_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "      # Remove tokens with cumulative probability above the threshold\n",
        "      sorted_idxs_to_remove = cumul_probs > top_p\n",
        "\n",
        "      # Shift the indices to the right to keep also the first token above the threshold\n",
        "      sorted_idxs_to_remove[..., 1:] = sorted_idxs_to_remove[..., :-1].clone()\n",
        "      sorted_idxs_to_remove[..., 0] = 0\n",
        "\n",
        "      # scatter sorted tensors to original indexing\n",
        "      indices_to_remove = sorted_idxs_to_remove.scatter(dim = 1, index = sorted_idxs, src = sorted_idxs_to_remove)\n",
        "      logits[indices_to_remove] = filter_amt\n",
        "\n",
        "  elif top_k > 0:\n",
        "      # Remove all tokens with a probability less than the last token of the top_k\n",
        "      indices_to_remove = logits < torch.topk(logits, int(top_k))[0][..., -1, None]\n",
        "      logits[indices_to_remove] = filter_amt\n",
        "\n",
        "  return logits"
      ],
      "metadata": {
        "id": "gqgR7sqjLzQt"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get output logits from paraphraser\n",
        "def get_logits(paraphraser, idx, sents, segments, past):\n",
        "  if idx == 0:\n",
        "      pred = paraphraser(input_ids = sents, token_type_ids = segments, return_dict=True)\n",
        "  else:\n",
        "      # used the cached representations to speed up decoding\n",
        "      pred = paraphraser(input_ids = sents[:, -1:], token_type_ids = segments[:, -1:], past_key_values = past, return_dict = True)\n",
        "\n",
        "  logits = pred['logits']\n",
        "  past_keys = pred['past_key_values']\n",
        "\n",
        "  return logits, past_keys"
      ],
      "metadata": {
        "id": "CH4F_VhwLvdQ"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decide generation lenght and get converted output and score\n",
        "def generate(paraphraser, sents_to_paraphrase, segments, eos_token_id, top_p, top_k, len_to_gen):\n",
        "  batch_size = sents_to_paraphrase.shape[0] # total sents in batch\n",
        "\n",
        "  eos_emitted = [False for _ in range(batch_size)]\n",
        "  scores = [{\"score\": 0, \"sequence\": []} for _ in range(batch_size)]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    past_keys = None\n",
        "\n",
        "    for i in range(len_to_gen):\n",
        "      op_logits, past_keys = get_logits(paraphraser, i, sents_to_paraphrase, segments, past_keys)\n",
        "      next_token_logits = op_logits[:, -1, :]\n",
        "      original_scores = F.log_softmax(next_token_logits, dim = -1)\n",
        "\n",
        "      # do nucleas filtering and greedy decoding\n",
        "      filtered_logits = top_k_top_p_filtering(next_token_logits, top_k = top_k, top_p = top_p)\n",
        "\n",
        "      if top_k in [0, 1] and top_p == 0.0: # mainly to control the output diversity\n",
        "        # greedy sampling\n",
        "        next_token = torch.argmax(filtered_logits, dim = -1).unsqueeze(-1)\n",
        "      else :\n",
        "        next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples = 1)\n",
        "\n",
        "      sents_to_paraphrase = torch.cat((sents_to_paraphrase, next_token), dim=1)\n",
        "      segments = torch.cat((segments, segments[:, -1:]), dim=1)\n",
        "\n",
        "      for batch_elem in range(batch_size):\n",
        "        if next_token[batch_elem].item() == eos_token_id:\n",
        "            eos_emitted[batch_elem] = True\n",
        "\n",
        "      if len_to_gen is None and all(eos_emitted):\n",
        "        break\n",
        "\n",
        "  return sents_to_paraphrase, scores"
      ],
      "metadata": {
        "id": "Iln1Z1oXLrR8"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess input from to paraphrase\n",
        "def preprocess(exp, tokenizer, config):\n",
        "  max_prefix_len = config[\"max_prefix_length\"]\n",
        "  sent1 = np.array(exp[\"sent1_tokens\"])\n",
        "\n",
        "  # truncate\n",
        "  if(len(sent1) > max_prefix_len):\n",
        "    sent1 = sent1[:max_prefix_len]\n",
        "\n",
        "  # add padding; left padding to prefix and right padding to suffix\n",
        "  count_pad_tokens_prefix = max_prefix_len - len(sent1)\n",
        "  sent1 = np.pad(sent1, (count_pad_tokens_prefix, 0), constant_values = tokenizer.pad_token_id)\n",
        "\n",
        "  # sentence to input gpt2\n",
        "  sentence_to_input_gpt2 = np.concatenate([sent1, [tokenizer.bos_token_id]]).astype(np.int64) # [sent1, <bos>]\n",
        "\n",
        "  # segment\n",
        "  segment = np.concatenate([\n",
        "      [tokenizer.additional_special_tokens_ids[0] for _ in sent1],\n",
        "      [tokenizer.additional_special_tokens_ids[1]]\n",
        "  ]).astype(np.int64)\n",
        "\n",
        "  exp[\"input\"] = sentence_to_input_gpt2\n",
        "  exp[\"segment\"] = segment\n",
        "\n",
        "  return exp"
      ],
      "metadata": {
        "id": "M5ZDGWraLNns"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate paraphrased sentences for batch of input sentences\n",
        "def generate_paraphrased_sent(paraphraser, upper_length, top_p, top_k, sents, config, device, tokenizer):\n",
        "  examples = []\n",
        "\n",
        "  for sent in sents:\n",
        "    token_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sent))\n",
        "    dd = {\"sent1_tokens\":token_ids}\n",
        "\n",
        "    dd_process = preprocess(dd, tokenizer, config)\n",
        "    examples.append(dd_process)\n",
        "\n",
        "  init_context_size = 1 + config[\"max_prefix_length\"]\n",
        "\n",
        "  # print(\"example to paraphrase-0 \", examples[0])\n",
        "\n",
        "  output, scores = generate(paraphraser,\n",
        "      sents_to_paraphrase = torch.tensor([inst[\"input\"] for inst in examples]).to(device),\n",
        "      segments = torch.tensor([inst[\"segment\"] for inst in examples]).to(device),\n",
        "      eos_token_id = tokenizer.eos_token_id,\n",
        "      top_p = top_p, top_k = top_k,\n",
        "      len_to_gen =  config[\"max_suffix_length\"] + 1  # +1 for <eos>\n",
        "  )\n",
        "\n",
        "  all_ops = []\n",
        "  for idx in range(len(output)):\n",
        "    exmp = examples[idx]\n",
        "    curr_out = output[idx, init_context_size:].tolist()\n",
        "\n",
        "    if tokenizer.eos_token_id in curr_out:\n",
        "      curr_out = curr_out[:curr_out.index(tokenizer.eos_token_id)]\n",
        "\n",
        "    if upper_length.startswith(\"same\"):\n",
        "      extra = int(upper_length.split(\"_\")[-1])\n",
        "      curr_out = curr_out[:len(exmp[\"sent1_tokens\"]) + extra]\n",
        "\n",
        "    all_ops.append(tokenizer.decode(curr_out, clean_up_tokenization_spaces = True, skip_special_tokens = True))\n",
        "\n",
        "  return all_ops, scores"
      ],
      "metadata": {
        "id": "bfzftBI2KHII"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load INVP for target style\n",
        "def load_inverse_paraphraser(target_style, device_no, invp_paraphrase_root_dir, model_class, tokenizer_class):\n",
        "\n",
        "  with torch.cuda.device(device_no):\n",
        "    print(\"Loading target style inverse paraphraser...\")\n",
        "    invp_chkpoint = invp_paraphrase_root_dir + \"/\" + target_style\n",
        "    invp_paraphraser = model_class.from_pretrained(invp_chkpoint)\n",
        "    invp_tokenizer = tokenizer_class.from_pretrained(invp_chkpoint, do_lower_case = True)\n",
        "\n",
        "    invp_paraphraser.to(torch.cuda.current_device())\n",
        "    print(\"Done loading target style Inverse Paraphraser..!!\")\n",
        "\n",
        "    return invp_paraphraser, invp_tokenizer"
      ],
      "metadata": {
        "id": "pZLwSh5tRysY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transfer_for_single_sentence(model_class, tokenizer_class, target_style, sents_to_transform,\n",
        "                                 invp_paraphrase_root_dir, invp_on_device,\n",
        "                                 dvp_paraphraser, dvp_tokenizer, dvp_on_device, config, device):\n",
        "\n",
        "  input_samples = sents_to_transform\n",
        "\n",
        "  # load inverse paraphraser\n",
        "  invp_paraphraser, invp_tokenizer = load_inverse_paraphraser(target_style, invp_on_device, invp_paraphrase_root_dir,\n",
        "                                                              model_class, tokenizer_class)\n",
        "  # For DVP\n",
        "  top_p_paraphrase = 0.0\n",
        "  top_k = 1\n",
        "\n",
        "  # Get outout from DVP\n",
        "  with torch.cuda.device(dvp_on_device):\n",
        "    dvp_outputs, _ = generate_paraphrased_sent(dvp_paraphraser,\"same_5\",\n",
        "                                            top_p_paraphrase, top_k,\n",
        "                                            input_samples, config, torch.cuda.current_device(), dvp_tokenizer)\n",
        "\n",
        "  # Pass dvp_output to loaded INVP\n",
        "  top_p_style = 0.7\n",
        "  top_k = 1\n",
        "  with torch.cuda.device(invp_on_device):\n",
        "    style_transferred_outputs, _ = generate_paraphrased_sent(invp_paraphraser, \"same_5\",\n",
        "                                                        top_p_style, top_k, dvp_outputs,\n",
        "                                                        config, torch.cuda.current_device(), invp_tokenizer)\n",
        "\n",
        "  return style_transferred_outputs # list of strings"
      ],
      "metadata": {
        "id": "QHAS6cbNOxxC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sent to transform and target style\n",
        "sent_to_transform = \"my name is ayush\"\n",
        "source_style = \"bible\" # btw; it does not even matter, only thing that matters is the target style; only used to get source style sample\n",
        "target_style = \"shakespeare\"\n",
        "\n",
        "style_transferred_sent = transfer_for_single_sentence(model_class, tokenizer_class,\n",
        "                                                      target_style, [sent_to_transform],\n",
        "                                                      final_invp_paraphrase_root_dir, invp_on_device,\n",
        "                                                      dvp_paraphraser, dvp_tokenizer, dvp_on_device, CONFIG, device)\n",
        "\n",
        "print(\"\\nSent [ORIGINAL]- \", sent_to_transform)\n",
        "print(\"Sent [\"+target_style+\"]- \", style_transferred_sent[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EufunHOQDuB",
        "outputId": "af0cd46c-ea39-4a58-fd58-1b4f3bf436c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading target style inverse paraphraser...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done loading target style Inverse Paraphraser..!!\n",
            "\n",
            "Sent [ORIGINAL]-  my name is ayush\n",
            "Sent [shakespeare]-  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for multiple sentences and to save them in file\n",
        "def do_style_transformation(from_style, to_style, src_data_samples_dir, data_samples_fname, style_transferred_dir):\n",
        "\n",
        "  # read source data\n",
        "  file_name_to_read = src_data_samples_dir + \"/\" + data_samples_fname[from_style]\n",
        "  with open(file_name_to_read, \"r\") as f:\n",
        "    data_orig = f.read().strip().split(\"\\n\")\n",
        "\n",
        "  data_to_convert = data_orig[:200]\n",
        "\n",
        "  batch_size = 50\n",
        "  style_transferred_sents = []\n",
        "\n",
        "  for i in tqdm.tqdm(range(0, len(data_to_convert), batch_size)):\n",
        "    sents_to_tranform = data_to_convert[i : i + batch_size]\n",
        "    style_transferred_sents.extend(sents_to_tranform)\n",
        "\n",
        "  # save style transferred sentences to file with the name <from_style>_To_<to_style>.txt in style_transferred_dir\n",
        "  file_path = style_transferred_dir + \"/\" + from_style + \"_To_\" + to_style + \".txt\"\n",
        "  with open(file_path, \"w\") as f:\n",
        "    f.writelines(\"%s\\n\" % item for item in style_transferred_sents)\n",
        "\n",
        "  print(f\"Transformation done for ({from_style}, {to_style})\")"
      ],
      "metadata": {
        "id": "2fWkTojEMGbS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Do for multiple sents pair;\n",
        "# target_style must be [\"shakespeare\", \"bible\", \"poetry\"] becoz INVP are only trained for them\n",
        "\n",
        "pairs_to_transform = [(\"bible\", \"shakespeare\"), (\"poetry\", \"shakespeare\"), (\"poetry\", \"bible\")] # (from, to)\n",
        "for (from_style, to_style) in pairs_to_transform:\n",
        "  do_style_transformation(from_style, to_style, src_data_samples_dir,\n",
        "                          src_data_samples_fname, final_style_transferred_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIdm-CYReTeA",
        "outputId": "c93ad48a-8ded-4c01-c0a2-c7cf3dcef1bd"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9709.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformation done for (bible, shakespeare)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inferencing done"
      ],
      "metadata": {
        "id": "cGBtYzkTgNr8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}