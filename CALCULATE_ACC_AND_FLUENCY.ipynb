{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GqCr3eL7YRfe"
      },
      "outputs": [],
      "source": [
        "# Get transfer accuracy and fluency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmVyADQDZCYO",
        "outputId": "386d50ca-2abe-46aa-bbdb-8c4297745901"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.5)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291605 sha256=31a4d381edcfe04154bc89f3ec41b35e21ef0c93f1386104d74b79e1bf26fd5b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=e5ea952ff416dfe1d8c54eb2f047d93940d84cc98f2025b451ab85ea4a2ebe45\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fairseq.models.roberta import RobertaModel\n",
        "from fairseq.data.data_utils import collate_tokens\n",
        "import torch\n",
        "import tqdm"
      ],
      "metadata": {
        "id": "miPvxBqdYfKA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# required dirs\n",
        "style_transfer_dir = \"/content/drive/MyDrive/Models/STYLE_TRANSFERRED\"\n",
        "acc_classifier_dir = \"/content/drive/MyDrive/Models/CLASSIFIER/SHAKESPEARE\"\n",
        "fluency_classifier_dir = \"/content/drive/MyDrive/Models/CLASSIFIER/FLUENCY/\"\n",
        "\n",
        "accepted_labels = {\n",
        "    \"shakespeare\" : \"original\"\n",
        "}\n",
        "accepted_fluency_label = \"acceptable\""
      ],
      "metadata": {
        "id": "i0oSepY-HZ_5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def label_fn(label, roberta):\n",
        "    return roberta.task.label_dictionary.string(\n",
        "        [label + roberta.task.target_dictionary.nspecial]\n",
        "    )"
      ],
      "metadata": {
        "id": "Tpaszk5kZLmm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize(x):\n",
        "    x = x.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\").replace(\" )\", \")\").replace(\"( \", \"(\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "Kzxyu-9-Zm0y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_acc(classifier_dir, bin_datadir, sents_list):\n",
        "  roberta = RobertaModel.from_pretrained(\n",
        "    classifier_dir,\n",
        "    checkpoint_file='checkpoint_best.pt',\n",
        "    data_name_or_path = bin_datadir\n",
        "  )\n",
        "\n",
        "  roberta.eval()\n",
        "  ncorrect, nsamples = 0, 0\n",
        "  roberta.cuda()\n",
        "\n",
        "  unk_bpe = roberta.bpe.encode(\" <unk>\").strip()\n",
        "  batch_size = 10\n",
        "\n",
        "  for i in tqdm.tqdm(range(0, len(sents_list), batch_size), total=len(sents_list) // batch_size):\n",
        "    sds = sents_list[i:i + batch_size]\n",
        "    sds = [roberta.bpe.encode(detokenize(sd.lower())) for sd in sds]\n",
        "    # lds = label_data[i:i + batch_size]\n",
        "\n",
        "    batch = collate_tokens(\n",
        "        [roberta.task.source_dictionary.encode_line(\"<s> \" + sd + \" </s>\", append_eos=False) for sd in sds], pad_idx=1\n",
        "    )\n",
        "\n",
        "    batch = batch[:, :512]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      predictions = roberta.predict('classification_head', batch.long())\n",
        "\n",
        "    prediction_labels = [label_fn(x.argmax(axis=0).item(), roberta) for x in predictions]\n",
        "\n",
        "  return prediction_labels[0]"
      ],
      "metadata": {
        "id": "cE2r9AU9YjPC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For accuracy\n",
        "acc_bin_datadir = '/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/'\n",
        "fluency_bin_datadir = '/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/cola-bin/'"
      ],
      "metadata": {
        "id": "SyvaSO8mKIIG"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read file\n",
        "from_style = \"bible\"\n",
        "to_style = \"shakespeare\"\n",
        "fname = style_transfer_dir + \"/sent_\"+from_style+\"_to_\"+to_style+\".txt\"\n",
        "\n",
        "with open(fname, \"r\") as f:\n",
        "  data = f.read().strip().split(\"\\n\")\n",
        "\n",
        "orig_seng = data[0]\n",
        "transferred_sent = data[1]\n",
        "\n",
        "print(f\"Orig Sent[{from_style}]- \", orig_seng)\n",
        "print(f\"Transferred Sent[{to_style}]- \", transferred_sent)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSkVkJIpYXvO",
        "outputId": "af4e237f-973a-484a-bcb6-7f5d82d2e734"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orig Sent[bible]-  And they caught him, and beat him, and sent him away empty.\n",
            "Transferred Sent[shakespeare]-  He was caught, throw him out.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label = get_acc(acc_classifier_dir, acc_bin_datadir, [transferred_sent]).strip().lower()\n",
        "print(\"\\nPredicted label- \", pred_label)\n",
        "\n",
        "if accepted_labels[to_style] != pred_label:\n",
        "  print(\"Not in target style\")\n",
        "  print(\"Accuracy : 0\")\n",
        "else:\n",
        "  print(\"In target style\")\n",
        "  print(\"Accuracy : 1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pa7GHrcOHJbW",
        "outputId": "bdc37c67-7c7e-4773-ea7f-438d479d6b00"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00, 57.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted label-  modern\n",
            "Not in target style\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For Fluency\n",
        "pred_label = get_acc(fluency_classifier_dir, fluency_bin_datadir, [transferred_sent]).strip().lower()\n",
        "print(\"\\nPredicted label- \", pred_label)\n",
        "\n",
        "if accepted_fluency_label != pred_label:\n",
        "  print(\"Not Fluent\")\n",
        "  print(\"Fluency : 0\")\n",
        "else:\n",
        "  print(\"Fleunt\")\n",
        "  print(\"Fluency : 1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS5zjGtTa_8o",
        "outputId": "4ae0b886-f2ec-4374-aac8-98a8c652456a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00, 28.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Predicted label-  acceptable\n",
            "Fleunt\n",
            "Fluency : 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label = get_acc(fluency_classifier_dir, fluency_bin_datadir, [transferred_sent]).strip().lower()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUD2olJTLO53",
        "outputId": "2376095b-df7e-4b93-e9ec-d2827279e83e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1it [00:00, 66.80it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "84ZKOQudOOvU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}