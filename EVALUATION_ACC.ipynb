{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VcqC2i0b75MQ"
      },
      "outputs": [],
      "source": [
        "# Evaluation\n",
        "# Need to do to calculate transfer accuracy, semantic similarity & Fluency"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fairseq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuNHYHiJKbNa",
        "outputId": "6baeb770-1e69-4557-a8c9-7e2bc69aaef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fairseq\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.16.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq) (3.0.5)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq)\n",
            "  Downloading sacrebleu-2.3.2-py3-none-any.whl (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.7/119.7 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fairseq) (4.66.1)\n",
            "Collecting bitarray (from fairseq)\n",
            "  Downloading bitarray-2.8.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (287 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.4/287.4 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq) (2.1.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq) (1.23.5)\n",
            "Collecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq) (2.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq) (2.21)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->fairseq) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291599 sha256=f70328e04468d3eb0a8b25e2f1684da2dd2b0fc7f2a5523f0410c8e071cfb824\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141210 sha256=3651536b5b60ade2bf448bd9f4ca7648aebc660ff4ebe7ed4d63f2a052b218e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq antlr4-python3-runtime\n",
            "Installing collected packages: bitarray, antlr4-python3-runtime, portalocker, omegaconf, colorama, sacrebleu, hydra-core, fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.8.3 colorama-0.4.6 fairseq-0.12.2 hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.8.2 sacrebleu-2.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxClV3zyUhd1",
        "outputId": "ac76b0e6-0bbb-4093-cdb5-9cd44c6afab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import torch\n",
        "from fairseq.data.data_utils import collate_tokens"
      ],
      "metadata": {
        "id": "Z3Hg-XH059T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\n",
        "!tar -xzvf roberta.base.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_u3Z5urQWe3",
        "outputId": "bd8a4751-2a4b-41f7-8105-85c981691b18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-20 10:43:40--  https://dl.fbaipublicfiles.com/fairseq/models/roberta.base.tar.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.19, 3.162.163.51, 3.162.163.11, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 231160875 (220M) [application/gzip]\n",
            "Saving to: ‘roberta.base.tar.gz’\n",
            "\n",
            "roberta.base.tar.gz 100%[===================>] 220.45M  50.2MB/s    in 4.5s    \n",
            "\n",
            "2023-11-20 10:43:45 (49.4 MB/s) - ‘roberta.base.tar.gz’ saved [231160875/231160875]\n",
            "\n",
            "roberta.base/\n",
            "roberta.base/dict.txt\n",
            "roberta.base/model.pt\n",
            "roberta.base/NOTE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -N 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxGNC-TKXCvI",
        "outputId": "0f5d5e24-e335-4339-f74f-19b951d27440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-20 10:43:50--  https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/dict.txt\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 3.162.163.19, 3.162.163.51, 3.162.163.11, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|3.162.163.19|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 603290 (589K) [text/plain]\n",
            "Saving to: ‘dict.txt’\n",
            "\n",
            "dict.txt            100%[===================>] 589.15K  2.73MB/s    in 0.2s    \n",
            "\n",
            "2023-11-20 10:43:51 (2.73 MB/s) - ‘dict.txt’ saved [603290/603290]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/train_0.txt\" \\\n",
        "    --validpref \"/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/dev_0.txt\" \\\n",
        "    --destdir \"custom_shakespeare-bin/input0\" \\\n",
        "    --workers 60 \\\n",
        "    --srcdict dict.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05PG2C3JXDQj",
        "outputId": "1f54d449-f4d7-4333-aa4b-78d3b7e66686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-20 08:37:53.669164: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 08:37:53.669226: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 08:37:53.669266: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 08:37:54.747160: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-20 08:37:57 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-11-20 08:37:57 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/train_0.txt', validpref='/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/dev_0.txt', testpref=None, align_suffix=None, destdir='custom_shakespeare-bin/input0', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict='dict.txt', nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, dict_only=False)\n",
            "2023-11-20 08:37:57 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2023-11-20 08:38:05 | INFO | fairseq_cli.preprocess | [None] /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/train_0.txt: 18395 sents, 235790 tokens, 92.2% replaced (by <unk>)\n",
            "2023-11-20 08:38:05 | INFO | fairseq_cli.preprocess | [None] Dictionary: 50264 types\n",
            "2023-11-20 08:38:10 | INFO | fairseq_cli.preprocess | [None] /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/dev_0.txt: 1218 sents, 14494 tokens, 91.6% replaced (by <unk>)\n",
            "2023-11-20 08:38:10 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to custom_shakespeare-bin/input0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!fairseq-preprocess \\\n",
        "    --only-source \\\n",
        "    --trainpref \"/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/train_0.attr\" \\\n",
        "    --validpref \"/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/dev_0.attr\" \\\n",
        "    --destdir \"custom_shakespeare-bin/label\" \\\n",
        "    --workers 60"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6a88o8T-XT7r",
        "outputId": "7f39ca92-1e57-4ecd-9495-9e376d30fc45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-20 08:38:37.570388: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 08:38:37.570461: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 08:38:37.570503: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 08:38:39.095142: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-20 08:38:42 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-11-20 08:38:42 | INFO | fairseq_cli.preprocess | Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='cross_entropy', tokenizer=None, bpe=None, optimizer=None, lr_scheduler='fixed', scoring='bleu', task='translation', source_lang=None, target_lang=None, trainpref='/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/train_0.attr', validpref='/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/dev_0.attr', testpref=None, align_suffix=None, destdir='custom_shakespeare-bin/label', thresholdtgt=0, thresholdsrc=0, tgtdict=None, srcdict=None, nwordstgt=-1, nwordssrc=-1, alignfile=None, dataset_impl='mmap', joined_dictionary=False, only_source=True, padding_factor=8, workers=60, dict_only=False)\n",
            "2023-11-20 08:38:44 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-20 08:38:45 | INFO | fairseq_cli.preprocess | [None] /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/train_0.attr: 18395 sents, 36790 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-20 08:38:45 | INFO | fairseq_cli.preprocess | [None] Dictionary: 8 types\n",
            "2023-11-20 08:38:47 | INFO | fairseq_cli.preprocess | [None] /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/dev_0.attr: 1218 sents, 2436 tokens, 0.0% replaced (by <unk>)\n",
            "2023-11-20 08:38:47 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to custom_shakespeare-bin/label\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 fairseq-train /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/ \\\n",
        "    --restore-file \"/content/roberta.base/model.pt\" \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size 8 \\\n",
        "    --max-tokens 4400 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --init-token 0 --separator-token 2 \\\n",
        "    --arch roberta_base \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --classification-head-name \"classification_head\" \\\n",
        "    --num-classes 2 \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr 1e-05 --total-num-update 7812 --warmup-updates 469 \\\n",
        "    --max-epoch 2 \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric \\\n",
        "    --shorten-method \"truncate\" \\\n",
        "    --find-unused-parameters \\\n",
        "    --update-freq 4 \\\n",
        "    --save-dir \"/content/drive/MyDrive/CLASSIFIER/SHAKESPEARE\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvJgawhMXc2a",
        "outputId": "33ec9a5d-8f6e-4f70-ea2f-74e3d0fda841"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-20 10:52:54.330091: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-20 10:52:54.330154: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-20 10:52:54.330202: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-20 10:52:55.549930: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-11-20 10:52:56 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2023-11-20 10:52:58 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2023-11-20 10:53:00 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': True, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4400, 'batch_size': 8, 'required_batch_size_multiple': 1, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4400, 'batch_size_valid': 8, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 2, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [4], 'lr': [1e-05], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/content/drive/MyDrive/CLASSIFIER/SHAKESPEARE', 'restore_file': '/content/roberta.base/model.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': False, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'accuracy', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(no_progress_bar=False, log_interval=100, log_format=None, log_file=None, aim_repo=None, aim_run_hash=None, tensorboard_logdir=None, wandb_project=None, azureml_logging=False, seed=1, cpu=False, tpu=False, bf16=False, memory_efficient_bf16=False, fp16=False, memory_efficient_fp16=False, fp16_no_flatten_grads=False, fp16_init_scale=128, fp16_scale_window=None, fp16_scale_tolerance=0.0, on_cpu_convert_precision=False, min_loss_scale=0.0001, threshold_loss_scale=None, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, user_dir=None, empty_cache_freq=0, all_gather_list_size=16384, model_parallel_size=1, quantization_config_path=None, profile=False, reset_logging=False, suppress_crashes=False, use_plasma_view=False, plasma_path='/tmp/plasma', criterion='sentence_prediction', tokenizer=None, bpe=None, optimizer='adam', lr_scheduler='polynomial_decay', scoring='bleu', task='sentence_prediction', num_workers=1, skip_invalid_size_inputs_valid_test=False, max_tokens=4400, batch_size=8, required_batch_size_multiple=1, required_seq_len_multiple=1, dataset_impl=None, data_buffer_size=10, train_subset='train', valid_subset='valid', combine_valid_subsets=None, ignore_unused_valid_subsets=False, validate_interval=1, validate_interval_updates=0, validate_after_updates=0, fixed_validation_seed=None, disable_validation=False, max_tokens_valid=4400, batch_size_valid=8, max_valid_steps=None, curriculum=0, gen_subset='test', num_shards=1, shard_id=0, grouped_shuffling=False, update_epoch_batch_itr=False, update_ordered_indices_seed=False, distributed_world_size=1, distributed_num_procs=1, distributed_rank=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, device_id=0, distributed_no_spawn=False, ddp_backend='pytorch_ddp', ddp_comm_hook='none', bucket_cap_mb=25, fix_batches_to_gpus=False, find_unused_parameters=True, gradient_as_bucket_view=False, fast_stat_sync=False, heartbeat_timeout=-1, broadcast_buffers=False, slowmo_momentum=None, slowmo_base_algorithm='localsgd', localsgd_frequency=3, nprocs_per_node=1, pipeline_model_parallel=False, pipeline_balance=None, pipeline_devices=None, pipeline_chunks=0, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_checkpoint='never', zero_sharding='none', no_reshard_after_forward=False, fp32_reduce_scatter=False, cpu_offload=False, use_sharded_state=False, not_fsdp_flatten_parameters=False, arch='roberta_base', max_epoch=2, max_update=0, stop_time_hours=0, clip_norm=0.0, sentence_avg=False, update_freq=[4], lr=[1e-05], stop_min_lr=-1.0, use_bmuf=False, skip_remainder_batch=False, save_dir='/content/drive/MyDrive/CLASSIFIER/SHAKESPEARE', restore_file='/content/roberta.base/model.pt', continue_once=None, finetune_from_model=None, reset_dataloader=True, reset_lr_scheduler=False, reset_meters=True, reset_optimizer=True, optimizer_overrides='{}', save_interval=1, save_interval_updates=0, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=-1, keep_best_checkpoints=-1, no_save=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_save_optimizer_state=False, best_checkpoint_metric='accuracy', maximize_best_checkpoint_metric=True, patience=-1, checkpoint_suffix='', checkpoint_shard_count=1, load_checkpoint_on_all_dp_ranks=False, write_checkpoints_asynchronously=False, store_ema=False, ema_decay=0.9999, ema_start_update=0, ema_seed_model=None, ema_update_freq=1, ema_fp32=False, encoder_layerdrop=0, encoder_layers_to_keep=None, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, spectral_norm_classification_head=False, min_params_to_wrap=100000000, mha_reg_scale_factor=0.0, ffn_reg_scale_factor=0.0, mha_heads_to_keep=-1, ffn_blocks_to_remove=-1, data='/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/', num_classes=2, init_token=0, separator_token=2, no_shuffle=False, shorten_method='truncate', shorten_data_split_list='', add_prev_output_tokens=False, classification_head_name='classification_head', regression_target=False, adam_betas='(0.9, 0.98)', adam_eps=1e-06, weight_decay=0.1, use_old_adam=False, fp16_adam_stats=False, warmup_updates=469, force_anneal=None, end_learning_rate=0.0, power=1.0, total_num_update='7812', pad=1, eos=2, unk=3, max_positions=512, dropout=0.1, attention_dropout=0.1, no_seed_provided=False, encoder_layers=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_attention_heads=12, activation_dropout=0.0, pooler_dropout=0.0, max_source_positions=512, no_token_positional_embeddings=False, encoder_learned_pos=True, layernorm_embedding=True, no_scale_embedding=True, activation_fn='gelu', encoder_normalize_before=False, pooler_activation_fn='tanh', untie_weights_roberta=False, adaptive_input=False, _name='roberta_base'), 'task': {'_name': 'sentence_prediction', 'data': '/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/', 'num_classes': 2, 'init_token': 0, 'separator_token': 2, 'no_shuffle': False, 'shorten_method': truncate, 'shorten_data_split_list': '', 'add_prev_output_tokens': False, 'max_positions': 512, 'regression_target': False, 'classification_head_name': 'classification_head', 'seed': 1}, 'criterion': {'_name': 'sentence_prediction', 'classification_head_name': 'classification_head', 'regression_target': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.1, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [1e-05]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 469, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 7812.0, 'lr': [1e-05]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}\n",
            "2023-11-20 10:53:01 | INFO | fairseq.tasks.sentence_prediction | [input] dictionary: 50265 types\n",
            "2023-11-20 10:53:01 | INFO | fairseq.tasks.sentence_prediction | [label] dictionary: 9 types\n",
            "2023-11-20 10:53:06 | INFO | fairseq_cli.train | RobertaModel(\n",
            "  (encoder): RobertaEncoder(\n",
            "    (sentence_encoder): TransformerEncoder(\n",
            "      (dropout_module): FairseqDropout()\n",
            "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
            "      (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
            "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "      (layers): ModuleList(\n",
            "        (0-11): 12 x TransformerEncoderLayerBase(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (dropout_module): FairseqDropout()\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (activation_dropout_module): FairseqDropout()\n",
            "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (lm_head): RobertaLMHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            "  (classification_heads): ModuleDict(\n",
            "    (classification_head): RobertaClassificationHead(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "      (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "2023-11-20 10:53:06 | INFO | fairseq_cli.train | task: SentencePredictionTask\n",
            "2023-11-20 10:53:06 | INFO | fairseq_cli.train | model: RobertaModel\n",
            "2023-11-20 10:53:06 | INFO | fairseq_cli.train | criterion: SentencePredictionCriterion\n",
            "2023-11-20 10:53:06 | INFO | fairseq_cli.train | num. shared model params: 210,306,395 (num. trained: 210,306,395)\n",
            "2023-11-20 10:53:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2023-11-20 10:53:06 | INFO | fairseq.data.data_utils | loaded 2,436 examples from: /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/input0/valid\n",
            "2023-11-20 10:53:07 | INFO | fairseq.data.data_utils | loaded 2,436 examples from: /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/label/valid\n",
            "2023-11-20 10:53:07 | INFO | fairseq.tasks.sentence_prediction | Loaded valid with #samples: 2436\n",
            "2023-11-20 10:53:14 | INFO | fairseq.trainer | detected shared parameter: encoder.sentence_encoder.embed_tokens.weight <- encoder.lm_head.weight\n",
            "2023-11-20 10:53:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-20 10:53:14 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.748 GB ; name = Tesla T4                                \n",
            "2023-11-20 10:53:14 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2023-11-20 10:53:14 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2023-11-20 10:53:14 | INFO | fairseq_cli.train | max tokens per device = 4400 and max sentences per device = 8\n",
            "2023-11-20 10:53:14 | INFO | fairseq.trainer | Preparing to load checkpoint /content/roberta.base/model.pt\n",
            "2023-11-20 10:53:17 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.classification_head.dense.weight\n",
            "2023-11-20 10:53:17 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.classification_head.dense.bias\n",
            "2023-11-20 10:53:17 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.classification_head.out_proj.weight\n",
            "2023-11-20 10:53:17 | INFO | fairseq.models.roberta.model | Overwriting classification_heads.classification_head.out_proj.bias\n",
            "2023-11-20 10:53:17 | INFO | fairseq.trainer | NOTE: your device may support faster training with --fp16 or --amp\n",
            "2023-11-20 10:53:18 | INFO | fairseq.trainer | Loaded checkpoint /content/roberta.base/model.pt (epoch 1 @ 0 updates)\n",
            "2023-11-20 10:53:18 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2023-11-20 10:53:18 | INFO | fairseq.data.data_utils | loaded 36,790 examples from: /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/input0/train\n",
            "2023-11-20 10:53:19 | INFO | fairseq.data.data_utils | loaded 36,790 examples from: /content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/label/train\n",
            "2023-11-20 10:53:19 | INFO | fairseq.tasks.sentence_prediction | Loaded train with #samples: 36790\n",
            "2023-11-20 10:53:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1150\n",
            "epoch 001:   0% 0/1150 [00:00<?, ?it/s]2023-11-20 10:53:19 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2023-11-20 10:53:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "/usr/local/lib/python3.10/dist-packages/fairseq/utils.py:374: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library\n",
            "  warnings.warn(\n",
            "epoch 001: 100% 1149/1150 [05:22<00:00,  3.99it/s, loss=0.429, nll_loss=0.03, accuracy=85.6, wps=1716.6, ups=3.71, wpb=463, bsz=32, num_updates=1100, lr=9.14068e-06, gnorm=10.749, train_wall=26, gb_free=12.3, wall=314]2023-11-20 10:58:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/305 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   0% 1/305 [00:00<00:38,  7.83it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   2% 7/305 [00:00<00:08, 34.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   4% 13/305 [00:00<00:06, 44.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   6% 19/305 [00:00<00:05, 50.21it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:   8% 25/305 [00:00<00:05, 49.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  10% 31/305 [00:00<00:05, 50.22it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  12% 37/305 [00:00<00:05, 51.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  14% 43/305 [00:00<00:05, 51.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  16% 49/305 [00:01<00:04, 52.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  18% 55/305 [00:01<00:04, 52.90it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  20% 61/305 [00:01<00:04, 52.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 67/305 [00:01<00:04, 48.78it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  24% 72/305 [00:01<00:04, 47.68it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 77/305 [00:01<00:04, 45.60it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  27% 82/305 [00:01<00:05, 41.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  29% 87/305 [00:01<00:05, 41.04it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  30% 92/305 [00:02<00:05, 41.23it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  32% 97/305 [00:02<00:05, 41.25it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 102/305 [00:02<00:04, 40.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  35% 107/305 [00:02<00:04, 41.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  37% 112/305 [00:02<00:04, 42.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  38% 117/305 [00:02<00:04, 43.10it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  40% 122/305 [00:02<00:04, 42.88it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  42% 127/305 [00:02<00:04, 41.49it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  43% 132/305 [00:02<00:04, 41.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  45% 137/305 [00:03<00:04, 40.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  47% 142/305 [00:03<00:03, 40.82it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  48% 147/305 [00:03<00:03, 40.79it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 152/305 [00:03<00:03, 41.16it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  51% 157/305 [00:03<00:03, 41.20it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  53% 162/305 [00:03<00:03, 40.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  55% 167/305 [00:03<00:03, 40.06it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 172/305 [00:03<00:03, 39.98it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  58% 177/305 [00:04<00:03, 39.80it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  59% 181/305 [00:04<00:03, 39.76it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  61% 186/305 [00:04<00:02, 40.14it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  63% 191/305 [00:04<00:02, 39.96it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  64% 196/305 [00:04<00:02, 40.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  66% 201/305 [00:04<00:02, 40.45it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  68% 206/305 [00:04<00:02, 39.84it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  69% 210/305 [00:04<00:02, 39.58it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  70% 214/305 [00:05<00:02, 38.62it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  72% 219/305 [00:05<00:02, 39.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  73% 224/305 [00:05<00:01, 40.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 229/305 [00:05<00:01, 41.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  77% 234/305 [00:05<00:01, 40.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 239/305 [00:05<00:01, 40.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  80% 244/305 [00:05<00:01, 40.41it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  82% 249/305 [00:05<00:01, 40.37it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 254/305 [00:05<00:01, 40.30it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  85% 259/305 [00:06<00:01, 40.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  87% 264/305 [00:06<00:01, 40.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  88% 269/305 [00:06<00:00, 41.61it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  90% 274/305 [00:06<00:00, 42.11it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  91% 279/305 [00:06<00:00, 41.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  93% 284/305 [00:06<00:00, 42.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  95% 289/305 [00:06<00:00, 42.33it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  96% 294/305 [00:06<00:00, 43.63it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  98% 299/305 [00:07<00:00, 44.26it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 305/305 [00:07<00:00, 46.60it/s]\u001b[A\n",
            "                                                                          \u001b[A2023-11-20 10:58:49 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 0.366 | nll_loss 0.027 | accuracy 88.1 | wps 4683.6 | wpb 108.3 | bsz 8 | num_updates 1150\n",
            "2023-11-20 10:58:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1150 updates\n",
            "2023-11-20 10:58:49 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/CLASSIFIER/SHAKESPEARE/checkpoint1.pt\n",
            "2023-11-20 10:59:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/CLASSIFIER/SHAKESPEARE/checkpoint1.pt\n",
            "2023-11-20 11:00:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/CLASSIFIER/SHAKESPEARE/checkpoint1.pt (epoch 1 @ 1150 updates, score 88.1) (writing took 75.52843712699996 seconds)\n",
            "2023-11-20 11:00:05 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2023-11-20 11:00:05 | INFO | train | epoch 001 | loss 0.55 | nll_loss 0.038 | accuracy 80.7 | wps 1327.4 | ups 2.86 | wpb 464.9 | bsz 32 | num_updates 1150 | lr 9.07259e-06 | gnorm 10.227 | train_wall 313 | gb_free 12.4 | wall 411\n",
            "2023-11-20 11:00:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1150\n",
            "epoch 002:   0% 0/1150 [00:00<?, ?it/s]2023-11-20 11:00:05 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2023-11-20 11:00:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002: 100% 1149/1150 [05:31<00:00,  3.62it/s, loss=0.344, nll_loss=0.024, accuracy=88.4, wps=1664.6, ups=3.59, wpb=463.5, bsz=32, num_updates=2200, lr=7.64265e-06, gnorm=10.207, train_wall=27, gb_free=12.2, wall=714]2023-11-20 11:05:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/305 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   0% 1/305 [00:00<00:47,  6.42it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   2% 5/305 [00:00<00:13, 21.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   3% 10/305 [00:00<00:10, 29.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   5% 15/305 [00:00<00:08, 33.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   6% 19/305 [00:00<00:08, 34.58it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   8% 23/305 [00:00<00:07, 35.64it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:   9% 28/305 [00:00<00:07, 37.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 33/305 [00:00<00:07, 37.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  12% 37/305 [00:01<00:07, 37.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  13% 41/305 [00:01<00:07, 36.68it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  15% 45/305 [00:01<00:07, 35.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  16% 49/305 [00:01<00:07, 33.98it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 53/305 [00:01<00:07, 34.00it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  19% 57/305 [00:01<00:07, 34.59it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  20% 61/305 [00:01<00:06, 34.97it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  21% 65/305 [00:01<00:06, 35.26it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  23% 69/305 [00:02<00:06, 35.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  24% 74/305 [00:02<00:05, 39.49it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  26% 79/305 [00:02<00:05, 41.95it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  28% 84/305 [00:02<00:05, 42.91it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  30% 90/305 [00:02<00:04, 46.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  31% 95/305 [00:02<00:04, 46.88it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 101/305 [00:02<00:04, 48.46it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  35% 106/305 [00:02<00:04, 48.75it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  37% 112/305 [00:02<00:03, 50.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  39% 118/305 [00:02<00:03, 50.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  41% 124/305 [00:03<00:03, 51.30it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  43% 130/305 [00:03<00:03, 49.82it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  45% 136/305 [00:03<00:03, 50.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  47% 142/305 [00:03<00:03, 50.96it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  49% 148/305 [00:03<00:03, 50.62it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 154/305 [00:03<00:02, 51.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  52% 160/305 [00:03<00:02, 51.17it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  54% 166/305 [00:03<00:02, 50.69it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 172/305 [00:04<00:02, 51.77it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  58% 178/305 [00:04<00:02, 51.29it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  60% 184/305 [00:04<00:02, 50.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  62% 190/305 [00:04<00:02, 50.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  64% 196/305 [00:04<00:02, 48.54it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  66% 201/305 [00:04<00:02, 48.78it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  68% 206/305 [00:04<00:02, 49.03it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  70% 212/305 [00:04<00:01, 49.79it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  71% 218/305 [00:04<00:01, 51.31it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  73% 224/305 [00:05<00:01, 51.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 230/305 [00:05<00:01, 50.71it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  77% 236/305 [00:05<00:01, 50.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  79% 242/305 [00:05<00:01, 50.84it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  81% 248/305 [00:05<00:01, 49.45it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 254/305 [00:05<00:01, 50.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  85% 260/305 [00:05<00:00, 50.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  87% 266/305 [00:05<00:00, 49.04it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 272/305 [00:06<00:00, 49.94it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  91% 278/305 [00:06<00:00, 50.93it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  93% 284/305 [00:06<00:00, 51.72it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  95% 290/305 [00:06<00:00, 50.11it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  97% 296/305 [00:06<00:00, 51.32it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  99% 302/305 [00:06<00:00, 50.18it/s]\u001b[A\n",
            "                                                                          \u001b[A2023-11-20 11:05:44 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 0.427 | nll_loss 0.032 | accuracy 86.2 | wps 5039.8 | wpb 108.3 | bsz 8 | num_updates 2300 | best_accuracy 88.1\n",
            "2023-11-20 11:05:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2300 updates\n",
            "2023-11-20 11:05:44 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/CLASSIFIER/SHAKESPEARE/checkpoint2.pt\n",
            "2023-11-20 11:05:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/CLASSIFIER/SHAKESPEARE/checkpoint2.pt\n",
            "2023-11-20 11:06:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint /content/drive/MyDrive/CLASSIFIER/SHAKESPEARE/checkpoint2.pt (epoch 2 @ 2300 updates, score 86.2) (writing took 45.386384461999796 seconds)\n",
            "2023-11-20 11:06:29 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2023-11-20 11:06:29 | INFO | train | epoch 002 | loss 0.367 | nll_loss 0.025 | accuracy 88.2 | wps 1392.1 | ups 2.99 | wpb 464.9 | bsz 32 | num_updates 2300 | lr 7.50647e-06 | gnorm 10.129 | train_wall 320 | gb_free 12.4 | wall 795\n",
            "2023-11-20 11:06:29 | INFO | fairseq_cli.train | done training in 790.4 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w2bqXz8zYIek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load checkpont\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    '/content/drive/MyDrive/CLASSIFIER/SHAKESPEARE',\n",
        "    checkpoint_file='checkpoint_best.pt',\n",
        "    data_name_or_path='/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare-bin/'\n",
        ")\n",
        "roberta.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxiBimioiVMf",
        "outputId": "13a091ee-bf10-43f4-9747-7ecc639d4dfc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1042301B [00:00, 4012647.47B/s]\n",
            "456318B [00:00, 10057257.93B/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (encoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x TransformerEncoderLayerBase(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict(\n",
              "      (classification_head): RobertaClassificationHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_fn(label):\n",
        "    return roberta.task.label_dictionary.string(\n",
        "        [label + roberta.task.target_dictionary.nspecial]\n",
        "    )"
      ],
      "metadata": {
        "id": "lVrVp-pBr-HD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp4-DLI4sS2I",
        "outputId": "c1509e3d-fb4a-4424-ff4e-cc45bfa05142"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaHubInterface(\n",
              "  (model): RobertaModel(\n",
              "    (encoder): RobertaEncoder(\n",
              "      (sentence_encoder): TransformerEncoder(\n",
              "        (dropout_module): FairseqDropout()\n",
              "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
              "        (embed_positions): LearnedPositionalEmbedding(514, 768, padding_idx=1)\n",
              "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (layers): ModuleList(\n",
              "          (0-11): 12 x TransformerEncoderLayerBase(\n",
              "            (self_attn): MultiheadAttention(\n",
              "              (dropout_module): FairseqDropout()\n",
              "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            )\n",
              "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout_module): FairseqDropout()\n",
              "            (activation_dropout_module): FairseqDropout()\n",
              "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (lm_head): RobertaLMHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "    (classification_heads): ModuleDict(\n",
              "      (classification_head): RobertaClassificationHead(\n",
              "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        (dropout): Dropout(p=0.0, inplace=False)\n",
              "        (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# do batch processing\n",
        "with open(\"/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/test_0.txt\", \"r\") as f:\n",
        "    author_data = f.read().strip().split(\"\\n\")\n",
        "\n",
        "print(author_data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK3N_-tjsVqL",
        "outputId": "c4161d1b-541f-4d36-c196-c333128dcf92"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riddling confession finds but riddling shrift .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/drive/MyDrive/IRE_Project/style_transfer_paraphrase/datasets/shakespeare/raw/test_0.attr\", \"r\") as f:\n",
        "    label_data = f.read().strip().split(\"\\n\")"
      ],
      "metadata": {
        "id": "Xb0mZI8FuP2f"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unk_bpe = roberta.bpe.encode(\" <unk>\").strip()\n",
        "batch_size = 10"
      ],
      "metadata": {
        "id": "K-NFycfbu6tw"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def detokenize(x):\n",
        "    x = x.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" !\", \"!\").replace(\" ?\", \"?\").replace(\" )\", \")\").replace(\"( \", \"(\")\n",
        "    return x"
      ],
      "metadata": {
        "id": "Gh66mV6wxsXx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm.tqdm(range(0, len(author_data), batch_size), total=len(author_data) // batch_size):\n",
        "    sds = author_data[i:i + batch_size]\n",
        "    lds = label_data[i:i + batch_size]\n",
        "\n",
        "    sds = [roberta.bpe.encode(detokenize(sd.lower())) for sd in sds]\n",
        "\n",
        "    batch = collate_tokens(\n",
        "        [roberta.task.source_dictionary.encode_line(\"<s> \" + sd + \" </s>\", append_eos=False) for sd in sds], pad_idx=1\n",
        "    )\n",
        "\n",
        "    batch = batch[:, :512]\n",
        "\n",
        "    with torch.no_grad():\n",
        "      predictions = roberta.predict('classification_head', batch.long())\n",
        "\n",
        "    prediction_labels = [label_fn(x.argmax(axis=0).item()) for x in predictions]\n",
        "    ncorrect += sum([1 if l1.lower() == l2.lower() else 0 for l1, l2 in zip(prediction_labels, lds)])\n",
        "    nsamples += len(sds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BW6wE5S-u8mn",
        "outputId": "d013c798-9eee-47f2-fe94-4843cda33fb3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "147it [00:03, 46.71it/s]                         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc = ncorrect/nsamples"
      ],
      "metadata": {
        "id": "suN26vS5yfBp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc"
      ],
      "metadata": {
        "id": "Fm8AWBMs1faH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f8f04c-93f3-4f34-9699-5fa0a2df5710"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8950068399452804"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xYoIP3kPb07y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}