{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VVpU-qzvPGnr"
      },
      "outputs": [],
      "source": [
        "# Structured Inverse Paraphraser\n",
        "# Very similar to DVP; except Dataset change"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------- IMP -------------\n",
        "# Run for dataset one-by-one in different kaggle account\n",
        "# While loading datasets make sure that label file has only 1 label\n",
        "# Remove +1 while calculating the average train loss from denominator\n",
        "# Remove global_step = 3 from saving the last model state\n",
        "# set limit_examples to None while creating custom dataset class"
      ],
      "metadata": {
        "id": "DzwSY6-NSdz9"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desired Imports\n",
        "import torch\n",
        "import tqdm\n",
        "from tqdm import trange\n",
        "from transformers import (AdamW, GPT2Config, GPT2LMHeadModel, GPT2Tokenizer, get_linear_schedule_with_warmup)\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "import os\n",
        "import shutil\n",
        "import subprocess\n",
        "import json\n",
        "import torch.nn.utils as F\n",
        "from transformers import WEIGHTS_NAME"
      ],
      "metadata": {
        "id": "aqs53SOMPXrp"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required directories\n",
        "invp_paraphrase_model_chkpts_dir =  \"/content/drive/MyDrive/IRE/INVP\"\n",
        "final_invp_paraphrase_model_dir = \"/content/drive/MyDrive/IRE/INVP/final_INVP\"\n",
        "input_output_dir = \"/content/drive/MyDrive/IRE/PSUEDO_PARALLEL\" # from DVP"
      ],
      "metadata": {
        "id": "BoSP4ybdP5ls"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset for inverse paraphrasing\n",
        "database_config = {\n",
        "    \"shakespeare\" : (\"shakespeare\", \"original\"), # (dir_name, style_label)\n",
        "    \"poetry\" : (\"poetry\", \"romantic_poetry\"),\n",
        "    \"bible\" : (\"bible\", \"bible\")\n",
        "}\n",
        "\n",
        "dataset_to_use = \"poetry\""
      ],
      "metadata": {
        "id": "BXsqHq9iQUjK"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments needed for training\n",
        "DIR_NAME = 0\n",
        "STYLE_LABEL = 1\n",
        "dataset_to_run = database_config[dataset_to_use][DIR_NAME]\n",
        "style_type = database_config[dataset_to_use][STYLE_LABEL]\n",
        "\n",
        "args_dir = {\n",
        "  \"save_steps\" : 20, # can be changed\n",
        "  \"num_epochs\" : 3,\n",
        "  \"gradient_accumulation_steps\" : 2,\n",
        "  \"adam_epsilon\" : 1e-8,\n",
        "  \"warmup_steps\" : 0,\n",
        "  \"learning_rt\" : 5e-5,\n",
        "  \"max_grad_norm\" : 1.0,\n",
        "  \"data_dir\" : input_output_dir + \"/\" + dataset_to_run + \"/BPE\",\n",
        "  \"model_type\" : \"gpt2\",\n",
        "  \"model_name\" : \"gpt2\",  # set to gtp2-large\n",
        "  \"train_batch_size\" : 5,\n",
        "  \"eval_batch_size\" : 5,\n",
        "  \"extra_embedding_dim\" : 768,\n",
        "  \"global_dense_feature_list\" : None # in file it will be saved with the value null; while reading take care of this thing\n",
        "}\n",
        "\n",
        "model_type = args_dir[\"model_type\"]\n",
        "model_name = args_dir[\"model_name\"]\n",
        "data_dir = args_dir[\"data_dir\"]\n",
        "save_steps = args_dir[\"save_steps\"]\n",
        "num_epochs = args_dir[\"num_epochs\"]\n",
        "gradient_accumulation_steps = args_dir[\"gradient_accumulation_steps\"]\n",
        "adam_epsilon = args_dir[\"adam_epsilon\"]\n",
        "warmup_steps = args_dir[\"warmup_steps\"]\n",
        "train_batch_size = args_dir[\"train_batch_size\"]\n",
        "eval_batch_size = args_dir[\"eval_batch_size\"]\n",
        "learning_rt = args_dir[\"learning_rt\"]\n",
        "extra_embedding_dim = args_dir[\"extra_embedding_dim\"] # Size of linear layer used for projecting extra embeddings.\n",
        "global_dense_feature_list = args_dir[\"global_dense_feature_list\"]\n",
        "max_grad_norm = args_dir[\"max_grad_norm\"]"
      ],
      "metadata": {
        "id": "L-MrCAwyROZc"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpus = torch.cuda.device_count()\n",
        "\n",
        "print(device, n_gpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPctOF2fTeuK",
        "outputId": "4c9f8578-9505-42b5-ecb8-a7576a7627ae"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsDkXVlfTmLT",
        "outputId": "a98720a1-c140-4a80-9425-c02321c30bae"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model classes variables\n",
        "MODEL_CLASSES = {\n",
        "    'gpt2': (GPT2Config, GPT2LMHeadModel, GPT2Tokenizer),\n",
        "}\n",
        "config_class, model_class, tokenizer_class = MODEL_CLASSES[model_type]\n",
        "\n",
        "print(\"GPT2 Config class- \", config_class)\n",
        "print(\"GPT2 Model class- \", model_class)\n",
        "print(\"GPT2 Tokenizer class- \", tokenizer_class)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YNkKRstToYq",
        "outputId": "053eac52-6285-45ff-ab1b-ffe4652bc1a8"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2 Config class-  <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'>\n",
            "GPT2 Model class-  <class 'transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel'>\n",
            "GPT2 Tokenizer class-  <class 'transformers.models.gpt2.tokenization_gpt2.GPT2Tokenizer'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Init config\n",
        "config = config_class.from_pretrained(model_name)\n",
        "print(\"GPT2Config loaded\")\n",
        "\n",
        "# Init model\n",
        "model = model_class.from_pretrained(model_name, config = config)\n",
        "print(\"GPT2LMHeadModel loaded\")\n",
        "\n",
        "# Init tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(model_name,do_lower_case = False)\n",
        "print(\"GPT2Tokenizer loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSlelhYlU5x_",
        "outputId": "a214e5af-6532-409e-d5fc-4089ef3f4e89"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2Config loaded\n",
            "GPT2LMHeadModel loaded\n",
            "GPT2Tokenizer loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# adding extra_dimension to config --- No need in DVP ---- Can be removed later\n",
        "config.extra_embedding_dim = extra_embedding_dim # don't know why we are using it; -- explore it"
      ],
      "metadata": {
        "id": "uXBOQCvCU88-"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add special tokens to tokenizer\n",
        "SPECIAL_TOKENS = {\n",
        "    \"additional_special_tokens\": [\"<segment_1>\", \"<segment_2>\"],\n",
        "    \"pad_token\": \"<pad>\",\n",
        "    \"bos_token\": \"<bos>\",\n",
        "    \"eos_token\": \"<eos>\"\n",
        "}\n",
        "tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
        "print(\"Special Tokens addded to tokenizer\")\n",
        "\n",
        "print(\"Total tokens- \", len(tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq8YQ-phVAoy",
        "outputId": "79eeece5-8e37-4350-da67-cf7645cbcefa"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special Tokens addded to tokenizer\n",
            "Total tokens-  50262\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# resize token embedding matrix to take care of special tokens added\n",
        "model.resize_token_embeddings(len(tokenizer)) # each token of size-> 1280(gpt2-large), 768(gpt2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkmyayHVVCxW",
        "outputId": "11cb50c8-cfe0-4c5b-b84a-ce713065be19"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50262, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# move model to device\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ksQybR3dVFd-",
        "outputId": "4b805a10-e7bb-4922-dbbe-2eecd18ec5c8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50262, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Init configs\n",
        "\n",
        "MAX_PARAPHRASE_LEN = 100\n",
        "\n",
        "# mainly to handle input\n",
        "CONFIG = {\n",
        "    \"max_prefix_len\": int(MAX_PARAPHRASE_LEN / 2),\n",
        "    \"max_suffix_len\": int(MAX_PARAPHRASE_LEN / 2)\n",
        "}"
      ],
      "metadata": {
        "id": "7l76yR5ak32e"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess input from paranmt\n",
        "def preprocess(exp, tokenizer, config):\n",
        "  MASK_TOKEN_ID = -100\n",
        "\n",
        "  max_prefix_len = config[\"max_prefix_len\"]\n",
        "  max_suffix_len = config[\"max_suffix_len\"]\n",
        "\n",
        "  sent1 = np.array(exp[\"prefix_sent\"])\n",
        "  sent2 = np.array(exp[\"actual_sent\"])\n",
        "\n",
        "  # truncate\n",
        "  if(len(sent1) > max_prefix_len):\n",
        "    sent1 = sent1[:max_prefix_len]\n",
        "\n",
        "  if(len(sent2) > max_suffix_len):\n",
        "    sent2 = sent2[:max_suffix_len]\n",
        "\n",
        "  # add padding; left padding to prefix and right padding to suffix\n",
        "  count_pad_tokens_prefix = max_prefix_len - len(sent1)\n",
        "  sent1 = np.pad(sent1, (count_pad_tokens_prefix, 0), constant_values = tokenizer.pad_token_id)\n",
        "\n",
        "  # add <eos> to suffix\n",
        "  sent2 = np.append(sent2, tokenizer.eos_token_id)\n",
        "\n",
        "  count_pad_tokens_suffix = (max_suffix_len + 1) - len(sent2)\n",
        "  sent2 = np.pad(sent2, (0, count_pad_tokens_suffix), constant_values = tokenizer.pad_token_id)\n",
        "\n",
        "  # sentence to input gpt2\n",
        "  sentence_to_input_gpt2 = np.concatenate([sent1, [tokenizer.bos_token_id], sent2]).astype(np.int64) # [sent1, <bos> sent2]\n",
        "\n",
        "  # label/gt to predict; -100 used for masking that input (in ground truth only)\n",
        "  gt = np.concatenate([\n",
        "      [MASK_TOKEN_ID for _ in sent1],\n",
        "      [MASK_TOKEN_ID],\n",
        "      [val if val != tokenizer.pad_token_id else MASK_TOKEN_ID for val in sent2]\n",
        "  ]).astype(np.int64)\n",
        "\n",
        "  # segment\n",
        "  segment = np.concatenate([\n",
        "      [tokenizer.additional_special_tokens_ids[0] for _ in sent1],\n",
        "      [tokenizer.additional_special_tokens_ids[1]],\n",
        "      [tokenizer.additional_special_tokens_ids[1] for _ in sent2]\n",
        "  ]).astype(np.int64)\n",
        "\n",
        "  exp[\"prefix_sent\"] = sent1\n",
        "  exp[\"suffix_sent\"] = sent2\n",
        "\n",
        "  exp[\"input\"] = sentence_to_input_gpt2\n",
        "  exp[\"label\"] = gt\n",
        "  exp[\"segment\"] = segment\n",
        "\n",
        "  return exp"
      ],
      "metadata": {
        "id": "XMn4DlmpXj7_"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INVP dataset\n",
        "class Inverse_Paraphraser_Dataset(Dataset):\n",
        "    def __init__(self, config, tokenizer, style_type, limit_examples = None, evaluate = False, split_type = \"train\"):\n",
        "      self.config = config\n",
        "      self.examples = []\n",
        "      self.label_dict, self.reverse_label_dict = {style_type : 0}, {0 : style_type}\n",
        "\n",
        "      file_to_read = data_dir + \"/\" + split_type + \".input0.bpe\"\n",
        "      with open(file_to_read) as f:\n",
        "        author_input_data = f.read().strip().split(\"\\n\")\n",
        "\n",
        "      file_to_read = data_dir + \"/\" + split_type + \".label\"\n",
        "      with open(file_to_read) as f:\n",
        "        suffix_styles = f.read().strip().split(\"\\n\")\n",
        "\n",
        "      file_to_read = data_dir + \"/\" + split_type + \".paraphrase_250_input0.bpe\"\n",
        "      with open(file_to_read) as f:\n",
        "        prefix_data = f.read().strip().split(\"\\n\")\n",
        "\n",
        "      print(\"Actual data(BPE)- \", author_input_data[0])\n",
        "      print(\"Paraphrased data(BPE)- \", prefix_data[0])\n",
        "      print(\"Suffix style- \", suffix_styles[0])\n",
        "\n",
        "      self.examples = []\n",
        "      for i, (act_sent, suffix_style) in tqdm.tqdm(enumerate(zip(author_input_data, suffix_styles)), total=len(author_input_data)):\n",
        "        self.examples.append({\n",
        "            \"prefix_sent\": np.array([int(x) for x in prefix_data[i].split()], dtype = np.int32),  # from DVP\n",
        "            \"actual_sent\": np.array([int(x) for x in act_sent.split()], dtype = np.int32),        # from actual\n",
        "            \"suffix_style\": self.label_dict[suffix_style],\n",
        "        })\n",
        "\n",
        "      if(limit_examples != None):\n",
        "        self.examples = self.examples[:limit_examples]\n",
        "\n",
        "      print(self.examples[0])\n",
        "\n",
        "      for eg in self.examples:\n",
        "        eg[\"original_style\"] = eg[\"suffix_style\"]\n",
        "\n",
        "      # perform same preprocess as that of DVP\n",
        "      self.examples = [preprocess(exp, tokenizer, config) for exp in self.examples]\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.examples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      sentence = self.examples[idx][\"input\"]\n",
        "      label = self.examples[idx][\"label\"]\n",
        "      segment = self.examples[idx][\"segment\"]\n",
        "\n",
        "      return {\n",
        "          \"sample_number\": idx,\n",
        "          \"sentence\": torch.tensor(sentence),\n",
        "          \"label\": torch.tensor(label),\n",
        "          \"segment\": torch.tensor(segment)\n",
        "      }"
      ],
      "metadata": {
        "id": "oNYpPxDhaQRZ"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create DVP dataset\n",
        "train_dataset = Inverse_Paraphraser_Dataset(CONFIG,\n",
        "                                            tokenizer,\n",
        "                                            style_type,\n",
        "                                            limit_examples = 10,\n",
        "                                            evaluate = False, split_type = \"train\")\n",
        "print(\"\\n\\n INVP Dataset created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujzW_OxalOkA",
        "outputId": "b8132ea4-53a5-426d-c842-88e742792d52"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual data(BPE)-  37 533 4053 11 1468 32586 338 30942 18209 11\n",
            "Paraphrased data(BPE)-  11274 12 16390 11 262 30942 10747 286 32586 11\n",
            "Suffix style-  romantic_poetry\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26880/26880 [00:01<00:00, 24941.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prefix_sent': array([11274,    12, 16390,    11,   262, 30942, 10747,   286, 32586,\n",
            "          11], dtype=int32), 'actual_sent': array([   37,   533,  4053,    11,  1468, 32586,   338, 30942, 18209,\n",
            "          11], dtype=int32), 'suffix_style': 0}\n",
            "\n",
            "\n",
            " INVP Dataset created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataloader\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler = train_sampler, batch_size = train_batch_size)\n",
        "\n",
        "print(\"INVP train dataloader created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fbDQJYywmkGU",
        "outputId": "b4c2350d-6753-4f5c-c305-f862cd9409c7"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INVP train dataloader created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjrWWuiTnMWM",
        "outputId": "03e62acb-b8a8-4530-b251-701ecfdb2c56"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sample_number': tensor([3, 8, 0, 7, 2]),\n",
              " 'sentence': tensor([[50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259,    40,  1101,  1654,   345,   821,   826,   546,   326,\n",
              "          50260,  5779,   531,    13, 50261, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259],\n",
              "         [50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,  4360,\n",
              "            314,  1101,   407,  3772,   351,   262,   366,  1537,  1865,   526,\n",
              "          50260,  1537,  1865,    11,  8805,   321,    11,   314,   466,   407,\n",
              "            588,   366,  1537,  1865,   526,   632,   857,   477,   323,   383,\n",
              "            922, 38177,    13, 50261, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259],\n",
              "         [50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "             40,  1101,  3612,   286,  9008,   345,   878,   345,  1561,    13,\n",
              "          50260,    40,   423,   257,  2000,   284,  5587, 17903,   304,   260,\n",
              "          14210,  2740,   338,    83,    13, 50261, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259],\n",
              "         [50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259,  5832,  1183,   787,   502,   257, 15807,    13,\n",
              "          50260, 12050, 17903,   257, 15807,   422,   502,    13, 50261, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259],\n",
              "         [50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 18454,   321,    11,   339,   338,  3734,    13,\n",
              "          50260, 18454,   321,    11,   339,   338,   880,    13, 50261, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259, 50259,\n",
              "          50259, 50259]]),\n",
              " 'label': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  5779,   531,    13, 50261,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100],\n",
              "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  1537,  1865,    11,  8805,   321,    11,   314,   466,   407,\n",
              "            588,   366,  1537,  1865,   526,   632,   857,   477,   323,   383,\n",
              "            922, 38177,    13, 50261,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100],\n",
              "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,    40,   423,   257,  2000,   284,  5587, 17903,   304,   260,\n",
              "          14210,  2740,   338,    83,    13, 50261,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100],\n",
              "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100, 12050, 17903,   257, 15807,   422,   502,    13, 50261,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100],\n",
              "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100, 18454,   321,    11,   339,   338,   880,    13, 50261,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
              "           -100,  -100]]),\n",
              " 'segment': tensor([[50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258],\n",
              "         [50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258],\n",
              "         [50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258],\n",
              "         [50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258],\n",
              "         [50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258, 50258,\n",
              "          50258, 50258]])}"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now follow the same as done in DVP"
      ],
      "metadata": {
        "id": "FO4NE9q7nUd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total steps needed\n",
        "t_total = len(train_dataloader) // gradient_accumulation_steps * num_epochs\n",
        "\n",
        "# setting up the optimizer & learning rate schedulers\n",
        "no_decay = ['bias', 'LayerNorm.weight', 'layer_norm.weight']\n",
        "grouped_parameters = [\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters()],\n",
        "        'weight_decay': 0.0\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = AdamW(grouped_parameters, lr = float(learning_rt), eps = adam_epsilon)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = warmup_steps, num_training_steps = t_total)\n",
        "\n",
        "print(\"Adam Optimizer and learning rate scheduler instantiated\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYa_1EdHoNwd",
        "outputId": "ba4bd68b-6218-4209-dcfc-a9d3cf4dce2d"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adam Optimizer and learning rate scheduler instantiated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training meta Information\n",
        "print(\"Num of examples- \", len(train_dataset))\n",
        "print(\"Num of epochs- \", num_epochs)\n",
        "print(\"Batch size- \", train_batch_size)\n",
        "print(\"Gradient acculmulation steps- \", gradient_accumulation_steps)\n",
        "print(\"Total optimization steps- \", t_total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4ImlIyUoOTm",
        "outputId": "8b26f895-423e-4db6-c816-8b30d7f97d75"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of examples-  10\n",
            "Num of epochs-  3\n",
            "Batch size-  5\n",
            "Gradient acculmulation steps-  2\n",
            "Total optimization steps-  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# zero out all the gradients\n",
        "model.zero_grad()"
      ],
      "metadata": {
        "id": "A4qZ0jOOoQ1X"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fn to save checkpoints\n",
        "def save_model(model, tokenizer, chkpt_dir, global_step, args_dir):\n",
        "  if not os.path.exists(chkpt_dir):\n",
        "    os.makedirs(chkpt_dir)\n",
        "  print(\"Directory created for new checkpt to save\")\n",
        "\n",
        "  model.save_pretrained(chkpt_dir)\n",
        "  tokenizer.save_pretrained(chkpt_dir)\n",
        "  print(\"Model and tokenizer saved\")\n",
        "\n",
        "  # save training arguments also\n",
        "  with open(chkpt_dir + \"/my_args.json\", \"w\") as json_file:\n",
        "    json.dump(args_dir, json_file)\n",
        "  print(\"Training arguments saved\")\n",
        "\n",
        "  with open(os.path.join(chkpt_dir, \"global_step.txt\"), \"w\") as f:\n",
        "    f.write(str(global_step) + \"\\n\")\n",
        "  print(\"Global step file saved\")\n",
        "\n",
        "  print(\"Checkpint saving process done..\")"
      ],
      "metadata": {
        "id": "bq5Dt3jnoTEX"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "global_step = 0\n",
        "train_loss_val = 0.0\n",
        "chkpts_dir_name = []\n",
        "\n",
        "# start training\n",
        "train_iterator = trange(int(num_epochs), desc = \"Epoch\")\n",
        "for epoch in train_iterator:\n",
        "    epoch_iterator = tqdm(train_dataloader)\n",
        "\n",
        "    for batch_idx, batch in enumerate(epoch_iterator):\n",
        "      sentences = batch[\"sentence\"].to(device)\n",
        "      labels = batch[\"label\"].to(device)\n",
        "      segments = batch[\"segment\"].to(device)\n",
        "      model.train()\n",
        "\n",
        "      outputs = model(input_ids=sentences, token_type_ids = segments, labels = labels)\n",
        "      # print(\"Got logits and loss\")\n",
        "\n",
        "      loss = outputs.loss\n",
        "      loss = loss / gradient_accumulation_steps\n",
        "      train_loss_val += loss.item()\n",
        "\n",
        "      loss.backward()\n",
        "\n",
        "      if (((batch_idx + 1) % gradient_accumulation_steps) == 0):\n",
        "        # print(\"Moved 1 step\")\n",
        "        F.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        model.zero_grad()\n",
        "        global_step += 1\n",
        "\n",
        "        if (global_step % save_steps == 0):\n",
        "          # save checkpoint here\n",
        "          print(\"Saving new checkpoint\")\n",
        "          chkpt_dir = invp_paraphrase_model_chkpts_dir + \"/\" + dataset_to_run + \"/invp_chkpt_\"+str(global_step)\n",
        "          chkpts_dir_name.append(\"invp_chkpt_\"+str(global_step))\n",
        "\n",
        "          save_model(model, tokenizer, chkpt_dir, global_step, args_dir)\n",
        "      # break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WEG_uejoihQ",
        "outputId": "a8c16d95-9af9-4ac7-d01b-cdd573054977"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
            "  0%|          | 0/2 [00:20<?, ?it/s]\n",
            "Epoch:  33%|███▎      | 1/3 [00:20<00:41, 20.75s/it]\n",
            "  0%|          | 0/2 [00:19<?, ?it/s]\n",
            "Epoch:  67%|██████▋   | 2/3 [00:39<00:19, 19.86s/it]\n",
            "  0%|          | 0/2 [00:17<?, ?it/s]\n",
            "Epoch: 100%|██████████| 3/3 [00:57<00:00, 19.02s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Average train_loss per step\n",
        "global_step, tr_loss = global_step, train_loss_val / (global_step + 1) # +1 only while testing\n",
        "print(\"Final Global step- \", global_step)\n",
        "print(\"Average training loss per step- \", tr_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5qclQkzmpo50",
        "outputId": "14d37889-2327-4f8b-d9b8-fd42dd63e384"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Global step-  0\n",
            "Average training loss per step-  111.62308502197266\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the last model also\n",
        "global_step = 3  # remove this also; fetch the last global step value\n",
        "chkpt_dir = invp_paraphrase_model_chkpts_dir + \"/\" + dataset_to_run + \"/invp_chkpt_\"+str(global_step)\n",
        "chkpts_dir_name.append(\"invp_chkpt_\"+str(global_step))\n",
        "\n",
        "save_model(model, tokenizer, chkpt_dir, global_step, args_dir)\n",
        "\n",
        "print(\"Last model state saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEkZaqmDpqY0",
        "outputId": "fcfc04b7-9530-4554-b5e6-d7fa8dcfcd32"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory created for new checkpt to save\n",
            "Model and tokenizer saved\n",
            "Training arguments saved\n",
            "Global step file saved\n",
            "Checkpint saving process done..\n",
            "Last model state saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Till now,\n",
        "# INVP trained on chosen dataset -> checkpoints saved -> Last model state saved\n",
        "print(\"Checkpoints saved with the name- \", chkpts_dir_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ucYD9kGpsS-",
        "outputId": "82589574-a24b-4a43-ae0c-b00ae8ac7e4b"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoints saved with the name-  ['invp_chkpt_3']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start Evaluation\n",
        "print(\"Starting Evaluation of INVP on dev data based on perplexity\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6-rHZ27p9vS",
        "outputId": "4916adba-135e-49f4-cd59-76a06bb58698"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Evaluation of INVP on dev data based on perplexity\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get validation dataset and dataloader\n",
        "import tqdm\n",
        "val_dataset = Inverse_Paraphraser_Dataset(CONFIG,\n",
        "                                          tokenizer,\n",
        "                                          style_type,\n",
        "                                          limit_examples = 10,\n",
        "                                          evaluate = True, split_type = \"dev\")\n",
        "\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_dataloader = DataLoader(val_dataset, sampler = val_sampler, batch_size = eval_batch_size)\n",
        "\n",
        "print(\"Validation dataset and dataloader created\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vn76Wfj9qAZJ",
        "outputId": "d07c6208-9cdd-4da1-b30e-c96ac8fc688f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actual data(BPE)-  37 533 4053 11 1468 32586 338 30942 18209 11\n",
            "Paraphrased data(BPE)-  11274 12 16390 11 262 30942 10747 286 32586 11\n",
            "Suffix style-  romantic_poetry\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26880/26880 [00:00<00:00, 58435.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'prefix_sent': array([11274,    12, 16390,    11,   262, 30942, 10747,   286, 32586,\n",
            "          11], dtype=int32), 'actual_sent': array([   37,   533,  4053,    11,  1468, 32586,   338, 30942, 18209,\n",
            "          11], dtype=int32), 'suffix_style': 0}\n",
            "Validation dataset and dataloader created\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation meta Information\n",
        "print(\"Num of examples- \", len(val_dataset))\n",
        "print(\"Batch size- \", eval_batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn-BsWSBqXCu",
        "outputId": "5effc4ba-b48d-46d1-da4f-b2fe7f0faecf"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num of examples-  10\n",
            "Batch size-  5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fn to evaluate on a DVP checkpoint\n",
        "def evaluate(model, tokenizer, chkpt_dir_name, val_dataloader):\n",
        "  val_loss = 0.0\n",
        "  model.eval()\n",
        "\n",
        "  for i, batch in enumerate(val_dataloader):\n",
        "    sentences = batch[\"sentence\"].to(device)\n",
        "    labels = batch[\"label\"].to(device)\n",
        "    segments = batch[\"segment\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      op = model(input_ids=sentences, token_type_ids=segments, labels=labels)\n",
        "      loss_val = op.loss.item()\n",
        "\n",
        "    val_loss += loss_val\n",
        "    # break\n",
        "\n",
        "  avg_val_loss = val_loss / (i + 1) # per batch average loss\n",
        "  perplexity = torch.exp(torch.tensor(avg_val_loss)) # perplexity of exp(avg_loss)\n",
        "\n",
        "  return perplexity"
      ],
      "metadata": {
        "id": "6y_EzEJfqZwa"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chkpts_dir_name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2-EFGHkqys7",
        "outputId": "bcc39364-0bf5-4190-90fc-00f2f4abc9b3"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['invp_chkpt_3']"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start evaluating the checkpoints on dev data and using perplexity as a measure\n",
        "perplexity_list = []\n",
        "\n",
        "for chkpt_name in chkpts_dir_name:\n",
        "  # load chkpoint\n",
        "  chkpt_to_load = invp_paraphrase_model_chkpts_dir + \"/\" + dataset_to_run + \"/\" + chkpt_name\n",
        "  model = model_class.from_pretrained(chkpt_to_load)\n",
        "  tokenizer = tokenizer_class.from_pretrained(chkpt_to_load, do_lower_case = True)\n",
        "  model.to(device)\n",
        "  print(\"Checkpoint- \" + chkpt_name + \" loaded\")\n",
        "\n",
        "  # evaluate loaded\n",
        "  print(\"Evaluating on loaded checkpoint\")\n",
        "  perplexity = evaluate(model, tokenizer, chkpt_dir, val_dataloader)\n",
        "  perplexity_list.append((chkpt_name, perplexity))\n",
        "\n",
        "print(\"INVP evaluated on all the saved checkpoints\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_aQmUjhVqZ7h",
        "outputId": "6bd24970-5221-4f4e-9096-61d5011e90ea"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint- invp_chkpt_3 loaded\n",
            "Evaluating on loaded checkpoint\n",
            "INVP evaluated on all the saved checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort perplexity list in increasing order to get best model\n",
        "perplexity_list.sort(key=lambda x: x[1].item())\n",
        "top_chkpt_name = perplexity_list[0][0]\n",
        "\n",
        "print(\"Top performing checkpoint is- \", top_chkpt_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9ff8E0iqaGh",
        "outputId": "d1277726-0be3-4be4-d373-3274ac2e7ee7"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top performing checkpoint is-  invp_chkpt_3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation on dev data done"
      ],
      "metadata": {
        "id": "a_0bYHZiqaNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move top performing checkpoint to other final_paraphrase_model dir\n",
        "copy_cmd = \"cp {}/* {}\".format(invp_paraphrase_model_chkpts_dir + \"/\" + dataset_to_run + \"/\" + top_chkpt_name,\n",
        "                               final_invp_paraphrase_model_dir + \"/\" + dataset_to_run)\n",
        "print(copy_cmd)\n",
        "\n",
        "# do copy to some other location\n",
        "subprocess.check_output(copy_cmd, shell=True)\n",
        "print(\"Copied successfully..!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHkThHtRrAar",
        "outputId": "7eedafde-7b57-4bcf-a06f-d0b9dababf04"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp /content/drive/MyDrive/IRE/INVP/poetry/invp_chkpt_3/* /content/drive/MyDrive/IRE/INVP/final_INVP/poetry\n",
            "Copied successfully..!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# just to verify that copied model is loading correctly or not\n",
        "chkpt_to_load = final_invp_paraphrase_model_dir + \"/\" + dataset_to_run\n",
        "model = model_class.from_pretrained(chkpt_to_load)\n",
        "tokenizer = tokenizer_class.from_pretrained(chkpt_to_load, do_lower_case = True)\n",
        "\n",
        "print(\"Model loaded successfully..!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4wn0HM9JrAfT",
        "outputId": "32788e38-cf79-4cb2-e6d6-b4989d311c49"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully..!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finish Inverse Paraphraser"
      ],
      "metadata": {
        "id": "6w7VBsN9rAjJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}